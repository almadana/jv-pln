{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd62487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/perezoso/anaconda3/envs/jv-pln/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import RobertaForMaskedLM, AutoTokenizer\n",
    "from os import listdir\n",
    "from scipy.io import savemat, loadmat\n",
    "import numpy as np\n",
    "import process_textGrid as tg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1423a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#load RoBERTa model\n",
    "modelname = 'roberta-base' \n",
    "\n",
    "roberta= RobertaForMaskedLM.from_pretrained(modelname)\n",
    "tokenizer= AutoTokenizer.from_pretrained(modelname)\n",
    "roberta.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d957db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab030ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e1f3aa74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50264"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dcef01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c18c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% main processing function, get predicted probabilities for each word\n",
    "#compare the predicted vector for final token at the end of the sentence, with actual encoded vector\n",
    "# sentences: list of sentences. each sentence should be a list of words\n",
    "# sliding: if 0, attempts to use every previous word as context for word i. if not, use only the indicated number of previous words\n",
    "def processSentence(sentences,sliding=0):\n",
    "    #print(sentence)\n",
    "    word_probs = []\n",
    "    b=np.repeat(a,2)\n",
    "    tokens = torch.tensor(tokenizer.encode(sentences)).unsqueeze(0) # get tokens from CamemBERT\n",
    "    print(tokens.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0663dd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None], dtype=object)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_markings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1e0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_array(sentence):\n",
    "    mask = \"<mask>\"\n",
    "    rWords = range(len(sentence))\n",
    "    a=[np.array(sentence) for i in rWords]\n",
    "    b=[ [a[i][j] if j<i else \"\" for j in rWords ] for i in rWords]\n",
    "    square =  [[mask if j==i else b[i][j] for j in rWords ] for i in rWords]\n",
    "    masked_array = [\" \".join(square[i]).strip() for i in rWords]\n",
    "    return(masked_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "fa1ce9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=mask_array(text_markings[0:10])\n",
    "tokens = tokenizer(a,padding=True)['input_ids']\n",
    "tokens_tensor=torch.tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "cd1db1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<mask>',\n",
       " 'increase <mask>',\n",
       " 'increase its <mask>',\n",
       " 'increase its size <mask>',\n",
       " 'increase its size fivefold <mask>',\n",
       " 'increase its size fivefold or <mask>',\n",
       " 'increase its size fivefold or tenfold <mask>',\n",
       " 'increase its size fivefold or tenfold give <mask>',\n",
       " 'increase its size fivefold or tenfold give it <mask>',\n",
       " 'increase its size fivefold or tenfold give it strength <mask>']"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57e8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_textGrid.py\n",
    "textGrid_folder = \"/home/perezoso/Dropbox/projects/jaco/Project ECOSud/JulesVerne/jv-pln/data/wav/revised/\"\n",
    "files = listdir(textGrid_folder)\n",
    "\n",
    "\n",
    "filenames = np.array(list((filter(lambda x: x.endswith(\".TextGrid\") , files))),dtype=object)\n",
    "phonemes = np.empty(filenames.shape,dtype=\"object\")\n",
    "words = np.empty(filenames.shape,dtype=\"object\")\n",
    "word_markings=np.empty(filenames.shape,dtype=\"object\")\n",
    "words=np.empty(filenames.shape,dtype=\"object\")\n",
    "phon_markings=np.empty(filenames.shape,dtype=\"object\")\n",
    "nWords=np.empty(filenames.shape,dtype=\"object\")\n",
    "\n",
    "\n",
    "sliding=10 #sliding parameter. if 0, will use ALL previous context... don't think it can handle more than 100... see CamemBERT documentation for max sentence size...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "358fe62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,file in enumerate(filenames):\n",
    "    all_markings = tg.parseTextGrid(textGrid_folder+file)\n",
    "    xmin,xmax,text_markings = zip(*all_markings[0]) #[0] text markings, [1] phoneme markings\n",
    "    #probs = processSentence(text_markings,sliding)\n",
    "    xminf = [float(x) for x in xmin]\n",
    "    xmaxf = [float(x) for x in xmax]\n",
    "    #probsf= [float(x) for x in probs]\n",
    "    word_markings[i]=np.array([xminf,xmaxf,np.repeat(0,len(xminf))]).transpose()\n",
    "    words[i]=np.array(text_markings)\n",
    "    nWords[i] = len(text_markings)\n",
    "    xmin,xmax,p_markings = zip(*all_markings[1]) #[0] text markings, [1] phoneme markings\n",
    "    xminf = [float(x) for x in xmin]\n",
    "    xmaxf = [float(x) for x in xmax]\n",
    "    phonemes[i]=p_markings\n",
    "    phon_markings[i]=np.array([xminf,xmaxf]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "c1ca19ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array(['certainly', 'might', 'possess', 'such', 'a', 'destructive',\n",
       "              'machine', 'and', 'in', 'these', 'disastrous', 'times', 'when',\n",
       "              'the', 'ingenuity', 'of', 'man', 'has', 'multiplied', 'the',\n",
       "              'power', 'of', 'weapons', 'of', 'war', 'it', 'was', 'possible',\n",
       "              'that', 'without', 'the', 'knowledge', 'of', 'others', 'a',\n",
       "              'state', 'to', 'work', 'such', 'a', 'formidable', 'engine', 'the',\n",
       "              'idea', 'of', 'a', 'war', 'machine', 'fell', 'before', 'the',\n",
       "              'declaration', 'of', 'governments', 'as', 'public', 'interest',\n",
       "              'was', 'in', 'question', 'and', 'transatlantic', 'communications',\n",
       "              'their', 'veracity', 'could', 'not', 'be', 'doubted', 'but', 'how',\n",
       "              'admit', 'that', 'the', 'construction', 'of', 'this', 'submarine',\n",
       "              'boat', 'had', 'escaped', 'the', 'public', 'eye', 'for', 'a',\n",
       "              'private', 'gentleman', 'to', 'keep', 'the', 'secret', 'under',\n",
       "              'such', 'circumstances', 'would', 'be', 'very', 'difficult', 'and',\n",
       "              'for', 'a', 'state', 'whose', 'every', 'act', 'is', 'persistently',\n",
       "              'watched', 'by', 'powerful', 'rivals', 'certainly', 'impossible',\n",
       "              'upon', 'my', 'arrival', 'in', 'new', 'york', 'several', 'persons',\n",
       "              'did', 'me', 'the', 'honor', 'of', 'consulting', 'the',\n",
       "              'phenomenon', 'in', 'question', 'i', 'had', 'published', 'in',\n",
       "              'france', 'a', 'work', 'in', 'two', 'volumes', 'entitled',\n",
       "              'mysteries', 'of', 'the', 'great', 'submarine', 'grounds'],\n",
       "             dtype='<U14')                                                       ,\n",
       "       array(['fortunately', 'this', 'compartment', 'did', 'not', 'halt', 'the',\n",
       "              'boilers', 'or', 'the', 'fires', 'would', 'have', 'been',\n",
       "              'immediately', 'extinguished', 'captain', 'anderson', 'ordered',\n",
       "              'the', 'engines', 'to', 'be', 'stopped', 'at', 'once', 'and',\n",
       "              'one', 'of', 'the', 'men', 'went', 'down', 'to', 'ascertain',\n",
       "              'the', 'extent', 'of', 'the', 'injury', 'some', 'minutes',\n",
       "              'afterwards', 'they', 'discovered', 'the', 'existence', 'of',\n",
       "              'hole', 'two', 'yards', 'in', 'diameter', 'in', 'the', 'ship',\n",
       "              \"'s\", 'bottom', 'such', 'a', 'leak', 'could', 'not', 'be',\n",
       "              'stopped', 'and', 'the', 'scotiha', 'her', 'paddles', 'half',\n",
       "              'submerged', 'was', 'obliged', 'to', 'continue', 'her', 'course',\n",
       "              'she', 'was', 'then', 'three', 'hundred', 'miles', 'from', 'cape',\n",
       "              'clear', 'and', 'after', 'three', 'days', 'delay', 'which',\n",
       "              'caused', 'great', 'uneasiness', 'in', 'liverpool', 'she',\n",
       "              'entered', 'the', 'basin', 'of', 'the', 'company', 'the',\n",
       "              'engineers', 'visited', 'the', 'scotiha', 'which', 'was', 'put',\n",
       "              'in', 'drydock', 'they', 'could', 'scarcely', 'believe', 'it',\n",
       "              'possible', 'at', 'two', 'yards', 'and', 'a', 'half', 'below',\n",
       "              'watermark', 'was', 'a', 'regular', 'rent', 'in', 'the', 'form',\n",
       "              'of', 'an', 'isosceles', 'triangle', 'the', 'broken', 'place',\n",
       "              'in', 'the', 'iron', 'plates', 'was', 'so', 'perfectly', 'defined',\n",
       "              'that', 'it', 'could', 'not', 'have', 'been', 'more', 'neatly',\n",
       "              'done', 'by'], dtype='<U12')                                       ,\n",
       "       array(['fifteen', 'days', 'later', 'two', 'thousand', 'miles', 'farther',\n",
       "              'off', 'the', 'helvetia', 'of', 'the', 'compagnie', 'nationale',\n",
       "              'and', 'the', 'shannon', 'of', 'the', 'royal', 'mail', 'steamship',\n",
       "              'company', 'sailing', 'to', 'wynwood', 'in', 'that', 'portion',\n",
       "              'of', 'the', 'atlantic', 'lying', 'between', 'the', 'united',\n",
       "              'states', 'and', 'europe', 'respectively', 'signalled', 'the',\n",
       "              'monster', 'to', 'each', 'other', 'in', 'forty', 'two', 'minutes',\n",
       "              'north', 'latitude', 'and', 'sixty', 'degrees', 'thirty', 'five',\n",
       "              'minutes', 'west', 'longitude', 'in', 'observations', 'they',\n",
       "              'thought', 'themselves', 'justified', 'in', 'estimating', 'the',\n",
       "              'minimum', 'length', 'of', 'the', 'mammal', 'at', 'more', 'than',\n",
       "              'three', 'hundred', 'and', 'fifty', 'feet', 'as', 'the', 'shannon',\n",
       "              'and', 'the', 'helvetia', 'were', 'of', 'similar', 'dimensions',\n",
       "              'than', 'it', 'though', 'they', 'measured', 'three', 'hundred',\n",
       "              'feet', 'overall', 'now', 'the', 'largest', 'whales', 'frequent',\n",
       "              'those', 'parts', 'of', 'the', 'sea', 'around', 'the', 'aleutian',\n",
       "              'cullomac', 'and', 'umgalic', 'islands', 'have', 'never',\n",
       "              'exceeded', 'the', 'length', 'of', 'sixty', 'yards', 'if', 'they',\n",
       "              'attain', 'that', 'in', 'every', 'place', 'of', 'great', 'resort',\n",
       "              'the', 'monster', 'was', 'the', 'fashion', 'they', 'sang', 'of',\n",
       "              'it', 'in', 'the', 'cafes', 'ridiculed', 'it', 'in', 'the',\n",
       "              'papers', 'and', 'represented', 'it', 'on', 'the', 'stage'],\n",
       "             dtype='<U12')                                                       ,\n",
       "       array(['it', 'was', 'clear', 'then', 'that', 'the', 'instrument',\n",
       "              'producing', 'the', 'perforation', 'was', 'not', 'common', 'stamp',\n",
       "              'and', 'after', 'having', 'been', 'driven', 'with', 'prodigious',\n",
       "              'strength', 'and', 'piercing', 'an', 'iron', 'plate', 'one', 'and',\n",
       "              'three', 'eighths', 'inches', 'thick', 'had', 'withdrawn',\n",
       "              'itself', 'by', 'a', 'backward', 'motion', 'such', 'was', 'the',\n",
       "              'last', 'fact', 'which', 'resulted', 'in', 'exciting', 'once',\n",
       "              'more', 'the', 'torrent', 'of', 'public', 'opinion', 'from',\n",
       "              'this', 'moment', 'all', 'unlucky', 'casualties', 'which', 'could',\n",
       "              'not', 'be', 'otherwise', 'accounted', 'for', 'were', 'put',\n",
       "              'down', 'to', 'the', 'monster', 'upon', 'this', 'imaginary',\n",
       "              'creature', 'rested', 'the', 'responsibility', 'of', 'all',\n",
       "              'these', 'shipwrecks', 'which', 'unfortunately', 'were',\n",
       "              'considerable', 'for', 'of', 'three', 'thousand', 'ships', 'whose',\n",
       "              'loss', 'was', 'annually', 'recorded', 'at', 'lloyds', 'the',\n",
       "              'number', 'of', 'sailing', 'and', 'steamships', 'supposed', 'to',\n",
       "              'be', 'totally', 'lost', 'from', 'the', 'absence', 'of', 'all',\n",
       "              'news', 'amounted', 'to', 'not', 'less', 'than', 'two', 'hundred',\n",
       "              'now', 'it', 'was', 'the', 'monster', 'who', 'justly', 'or',\n",
       "              'unjustly', 'was', 'accused', 'of', 'their', 'disappearance',\n",
       "              'and', 'thanks', 'to', 'it', 'communication', 'between', 'the',\n",
       "              'different', 'continents', 'became', 'more', 'and', 'more',\n",
       "              'dangerous'], dtype='<U14')                                        ,\n",
       "       array(['this', 'book', 'highly', 'approved', 'of', 'in', 'the', 'learned',\n",
       "              'world', 'gained', 'for', 'me', 'a', 'special', 'reputation', 'in',\n",
       "              'this', 'rather', 'obscure', 'branch', 'of', 'natural', 'history',\n",
       "              'my', 'advice', 'was', 'asked', 'as', 'long', 'as', 'i', 'could',\n",
       "              'deny', 'the', 'reality', 'of', 'the', 'fact', 'i', 'confined',\n",
       "              'myself', 'to', 'a', 'decided', 'negative', 'but', 'soon',\n",
       "              'finding', 'myself', 'driven', 'into', 'a', 'corner', 'i', 'was',\n",
       "              'obliged', 'to', 'explain', 'myself', 'point', 'by', 'point', 'i',\n",
       "              'discuss', 'the', 'question', 'in', 'all', 'its', 'forms',\n",
       "              'politically', 'and', 'scientifically', 'and', 'i', 'give', 'here',\n",
       "              'an', 'extra', 'extract', 'from', 'a', 'carefully', 'studied',\n",
       "              'article', 'which', 'i', 'published', 'in', 'of', 'the', '30th',\n",
       "              'of', 'april', 'it', 'ran', 'as', 'follows', 'after', 'examining',\n",
       "              'one', 'by', 'one', 'the', 'different', 'theories', 'rejecting',\n",
       "              'all', 'other', 'suggestions', 'it', 'becomes', 'necessary', 'to',\n",
       "              'admit', 'the', 'existence', 'of', 'a', 'marine', 'animal', 'of',\n",
       "              'enormous', 'power', 'the', 'great', 'depths', 'of', 'the',\n",
       "              'ocean', 'are', 'entirely', 'unknown', 'to', 'us', 'soundings',\n",
       "              'cannot', 'reach', 'them', 'what', 'passes', 'in', 'those',\n",
       "              'remote', 'depths', 'what', 'beings', 'live', 'or', 'can', 'live',\n",
       "              '12', 'or', '15', 'miles', 'beneath', 'the', 'surface', 'of',\n",
       "              'the', 'waters'], dtype='<U14')                                    ,\n",
       "       array(['year', '1866', 'was', 'signalised', 'by', 'a', 'remarkable',\n",
       "              'incident', 'a', 'mysterious', 'and', 'puzzling', 'phenomenon',\n",
       "              'which', 'doubtless', 'no', 'one', 'has', 'yet', 'forgotten',\n",
       "              'not', 'to', 'mention', 'rumours', 'the', 'maritime', 'population',\n",
       "              'and', 'excited', 'the', 'public', 'mind', 'even', 'in', 'the',\n",
       "              'interior', 'of', 'continents', 'men', 'were', 'particularly',\n",
       "              'excited', 'merchants', 'common', 'sailors', 'captains', 'of',\n",
       "              'vessels', 'skippers', 'both', 'of', 'europe', 'and', 'america',\n",
       "              'naval', 'officers', 'of', 'all', 'countries', 'and', 'the',\n",
       "              'governments', 'of', 'several', 'states', 'on', 'the', 'two',\n",
       "              'continents', 'were', 'deeply', 'interested', 'in', 'the',\n",
       "              'matter', 'for', 'some', 'time', 'past', 'vessels', 'had', 'been',\n",
       "              'met', 'by', 'an', 'enormous', 'thing', 'a', 'long', 'object',\n",
       "              'spindle', 'shaped', 'occasionally', 'phosphorescent', 'and',\n",
       "              'infinitely', 'larger', 'and', 'more', 'rapid', 'in', 'its',\n",
       "              'movements', 'than', 'a', 'whale', 'facts', 'relating', 'to',\n",
       "              'this', 'apparition', 'entered', 'in', 'various', 'logbooks',\n",
       "              'agreed', 'in', 'most', 'respects', 'as', 'to', 'the', 'shape',\n",
       "              'of', 'the', 'object', 'or', 'creature', 'in', 'question', 'the',\n",
       "              'untiring', 'rapidity', 'of', 'its', 'movement', 'its',\n",
       "              'surprising', 'power', 'of', 'locomotion', 'and', 'the',\n",
       "              'peculiar', 'life', 'with', 'which', 'it', 'seemed', 'to', 'be',\n",
       "              'moving', 'down'], dtype='<U14')                                   ,\n",
       "       array(['if', 'it', 'was', 'a', 'whale', 'it', 'surpassed', 'in', 'size',\n",
       "              'all', 'those', 'hitherto', 'classified', 'in', 'science',\n",
       "              'taking', 'the', 'mean', 'of', 'observations', 'made', 'at',\n",
       "              'divers', 'times', 'rejecting', 'the', 'timid', 'estimate', 'of',\n",
       "              'those', 'who', 'assigned', 'to', 'this', 'object', 'a', 'length',\n",
       "              'of', 'two', 'hundred', 'feet', 'the', 'which', 'set', 'it',\n",
       "              'down', 'as', 'a', 'mile', 'in', 'width', 'and', 'three', 'in',\n",
       "              'length', 'we', 'might', 'fairly', 'conclude', 'that', 'this',\n",
       "              'mysterious', 'being', 'surpassed', 'greatly', 'all', 'dimensions',\n",
       "              'admitted', 'by', 'the', 'learned', 'ones', 'of', 'the', 'day',\n",
       "              'if', 'it', 'existed', 'at', 'all', 'and', 'that', 'it', 'did',\n",
       "              'exist', 'was', 'an', 'undeniable', 'fact', 'and', 'with', 'that',\n",
       "              'tendency', 'which', 'disposes', 'the', 'human', 'mind', 'in',\n",
       "              'favour', 'of', 'the', 'marvellous', 'we', 'can', 'understand',\n",
       "              'the', 'excitement', 'produced', 'in', 'the', 'entire', 'world',\n",
       "              'by', 'this', 'supernatural', 'apparition', 'as', 'to', 'classing',\n",
       "              'it', 'in', 'the', 'list', 'of', 'fables', 'the', 'idea', 'was',\n",
       "              'out', 'of', 'the', 'question', 'on', 'the', '20th', 'of', 'july',\n",
       "              '1866', 'the', 'steamer', 'governor', 'higginson', 'of', 'the',\n",
       "              'calcutta', 'and', 'burnet', 'steam', 'navigation', 'company',\n",
       "              'had', 'met', 'this', 'moving', 'mass', 'five', 'miles', 'off',\n",
       "              'the', 'east', 'coast', 'of', 'australia'], dtype='<U12')          ,\n",
       "       array(['they', 'saw', 'nothing', 'but', 'a', 'strong', 'eddy', 'about',\n",
       "              'three', 'cables', 'length', 'distant', 'as', 'if', 'the',\n",
       "              'surface', 'had', 'been', 'violently', 'agitated', 'the',\n",
       "              'bearings', 'of', 'the', 'place', 'were', 'taken', 'exactly',\n",
       "              'the', 'moravian', 'continued', 'its', 'route', 'without',\n",
       "              'apparent', 'damage', 'had', 'it', 'struck', 'on', 'a',\n",
       "              'submerged', 'rock', 'or', 'on', 'an', 'enormous', 'wreck', 'they',\n",
       "              'could', 'not', 'tell', 'but', 'on', 'examination', 'of', 'the',\n",
       "              'ship', \"'s\", 'bottom', 'when', 'undergoing', 'repairs', 'it',\n",
       "              'was', 'found', 'that', 'part', 'of', 'her', 'keel', 'was',\n",
       "              'broken', 'this', 'fact', 'so', 'grave', 'in', 'itself', 'might',\n",
       "              'perhaps', 'have', 'been', 'forgotten', 'like', 'many', 'others',\n",
       "              'if', 'three', 'weeks', 'after', 'it', 'had', 'not', 'been', 're',\n",
       "              'enacted', 'under', 'similar', 'circumstances', 'but', 'thanks',\n",
       "              'to', 'the', 'nationality', 'of', 'the', 'victim', 'of', 'the',\n",
       "              'shock', 'thanks', 'to', 'the', 'reputation', 'of', 'the',\n",
       "              'company', 'to', 'which', 'the', 'vessel', 'belonged', 'the',\n",
       "              'circumstance', 'became', 'extensively', 'circulated', '13th',\n",
       "              'of', 'april', '1867', 'the', 'sea', 'being', 'beautiful', 'the',\n",
       "              'breeze', 'favourable', 'the', 'scotia', 'of', 'the', 'cunard',\n",
       "              'company', \"'s\", 'line', 'found', 'herself', \"in15°12'longitude\",\n",
       "              \"and45°37'latitude\"], dtype='<U17')                                ,\n",
       "       array(['what', 'is', 'the', 'organization', 'of', 'these', 'animals',\n",
       "              'we', 'can', 'scarcely', 'conjecture', 'however', 'the',\n",
       "              'solution', 'of', 'the', 'problem', 'submitted', 'to', 'me', 'may',\n",
       "              'modify', 'the', 'form', 'of', 'the', 'dilemma', 'either', 'we',\n",
       "              'do', 'know', 'all', 'the', 'varieties', 'of', 'beings', 'which',\n",
       "              'people', 'our', 'planet', 'or', 'we', 'do', 'not', 'if', 'we',\n",
       "              'do', 'not', 'know', 'them', 'all', 'if', 'nature', 'has', 'still',\n",
       "              'secrets', 'in', 'the', 'deepsvars', 'nothing', 'is', 'more',\n",
       "              'conformable', 'to', 'reason', 'than', 'to', 'admit', 'the',\n",
       "              'existence', 'of', 'fishes', 'or', 'cetaceans', 'of', 'other',\n",
       "              'kinds', 'or', 'even', 'of', 'new', 'species', 'of', 'an',\n",
       "              'organization', 'formed', 'to', 'inhabit', 'the', 'strata',\n",
       "              'inaccessible', 'to', 'soundings', 'and', 'which', 'an',\n",
       "              'accident', 'of', 'some', 'sort', 'at', 'long', 'intervals', 'to',\n",
       "              'the', 'upper', 'level', 'of', 'the', 'ocean', 'if', 'on', 'the',\n",
       "              'contrary', 'we', 'do', 'know', 'all', 'living', 'kinds', 'we',\n",
       "              'must', 'necessarily', 'seek', 'for', 'the', 'animal', 'in',\n",
       "              'question', 'amongst', 'those', 'marine', 'beings', 'already',\n",
       "              'classed', 'and', 'in', 'that', 'case', 'i', 'should', 'be',\n",
       "              'disposed', 'to', 'existence', 'of', 'a', 'gigantic', 'narwhal',\n",
       "              'the', 'common', 'narwhal', 'or', 'unicorn', 'of', 'the', 'sea',\n",
       "              'maintains', 'a', '60', 'feet'], dtype='<U12')                     ,\n",
       "       array(['she', 'was', 'going', 'at', 'the', 'speed', 'of', 'thirteen',\n",
       "              'knots', 'and', 'a', 'half', 'at', 'seventeen', 'minutes', 'past',\n",
       "              'four', 'in', 'the', 'afternoon', 'whilst', 'the', 'passengers',\n",
       "              'were', 'assembled', 'at', 'lunch', 'in', 'the', 'great', 'saloon',\n",
       "              'a', 'slight', 'shock', 'was', 'felt', 'on', 'the', 'hull', 'of',\n",
       "              'the', 'scotiour', 'on', 'her', 'quarter', 'a', 'little', 'the',\n",
       "              'port', 'paddle', 'scotiour', 'had', 'not', 'struck', 'but', 'she',\n",
       "              'had', 'been', 'struck', 'and', 'seemingly', 'by', 'something',\n",
       "              'rather', 'sharp', 'and', 'penetrating', 'than', 'blunt', 'the',\n",
       "              'shock', 'had', 'been', 'so', 'slight', 'that', 'no', 'one', 'had',\n",
       "              'been', 'alarmed', 'and', 'had', 'it', 'not', 'been', 'for', 'the',\n",
       "              'shouts', 'of', 'the', 'carpenter', \"'s\", 'watch', 'who', 'rushed',\n",
       "              'onto', 'the', 'bridge', 'exclaiming', 'we', 'are', 'sinking',\n",
       "              'we', 'are', 'sinking', 'at', 'first', 'the', 'passengers', 'were',\n",
       "              'much', 'frightened', 'but', 'captain', 'anderson', 'hastened',\n",
       "              'to', 'reassure', 'them', 'the', 'danger', 'could', 'not', 'be',\n",
       "              'imminent', 'the', 'scotiour', 'divided', 'into', 'seven',\n",
       "              'compartments', 'by', 'could', 'brave', 'with', 'impunity', 'any',\n",
       "              'leak', 'captain', 'anderson', 'went', 'down', 'immediately',\n",
       "              'into', 'the', 'hull', 'he', 'found', 'that', 'the', 'sea', 'was',\n",
       "              'pouring', 'into', 'the', 'fifth', 'compartment', 'and', 'the',\n",
       "              'rapidity', 'of', 'the', 'influx', 'proved', 'that', 'the',\n",
       "              'force', 'of', 'the', 'water', 'was', 'considerable'], dtype='<U12'),\n",
       "       array(['all', 'kinds', 'of', 'stories', 'were', 'circulated', 'regarding',\n",
       "              'it', 'there', 'appeared', 'in', 'the', 'papers', 'caricatures',\n",
       "              'of', 'every', 'gigantic', 'and', 'imaginary', 'creature', 'from',\n",
       "              'the', 'the', 'terrible', 'moby', 'dick', 'of', 'sub', 'arctic',\n",
       "              'regions', 'to', 'the', 'immense', 'kraken', 'whose', 'tentacles',\n",
       "              'could', 'entangle', 'a', 'ship', 'of', 'five', 'hundred', 'tons',\n",
       "              'and', 'hurry', 'it', 'into', 'the', 'abyss', 'of', 'the', 'ocean',\n",
       "              'the', 'legends', 'of', 'ancient', 'times', 'were', 'even',\n",
       "              'revived', 'then', 'burst', 'forth', 'the', 'unending', 'argument',\n",
       "              'between', 'the', 'believers', 'and', 'the', 'unbelievers', 'in',\n",
       "              'the', 'societies', 'of', 'the', 'wise', 'and', 'the',\n",
       "              'scientific', 'journals', 'the', 'question', 'of', 'the',\n",
       "              'monster', 'inflamed', 'all', 'minds', 'editors', 'of',\n",
       "              'scientific', 'journals', 'quarrelling', 'with', 'believers', 'in',\n",
       "              'the', 'supernatural', 'spilled', 'seas', 'of', 'ink', 'during',\n",
       "              'this', 'memorable', 'campaign', 'some', 'even', 'drawing',\n",
       "              'blood', 'for', 'from', 'the', 'sea', 'serpent', 'they', 'came',\n",
       "              'to', 'direct', 'personalities', 'during', 'the', 'first',\n",
       "              'months', 'of', 'the', 'year', '1867', 'the', 'question', 'seemed',\n",
       "              'buried', 'never', 'to', 'revive', 'when', 'new', 'facts', 'were',\n",
       "              'brought', 'before', 'the', 'public'], dtype='<U13')               ,\n",
       "       array(['the', 'public', 'demanded', 'sharply', 'that', 'the', 'seas',\n",
       "              'should', 'at', 'any', 'price', 'be', 'relieved', 'from', 'this',\n",
       "              'formidable', 'cetacean', 'at', 'the', 'period', 'when', 'these',\n",
       "              'events', 'took', 'place', 'i', 'had', 'just', 'returned', 'from',\n",
       "              'a', 'scientific', 'research', 'in', 'the', 'disagreeable',\n",
       "              'territory', 'of', 'nebraska', 'in', 'the', 'united', 'states',\n",
       "              'in', 'virtue', 'of', 'my', 'office', 'as', 'assistant',\n",
       "              'professor', 'in', 'the', 'museum', 'of', 'in', 'paris', 'the',\n",
       "              'french', 'government', 'had', 'attached', 'me', 'to', 'that',\n",
       "              'expedition', 'after', 'six', 'months', 'in', 'nebraska', 'i',\n",
       "              'arrived', 'in', 'new', 'york', 'towards', 'the', 'end', 'of',\n",
       "              'march', 'laden', 'with', 'a', 'precious', 'collection', 'my',\n",
       "              'departure', 'for', 'france', 'was', 'fixed', 'for', 'the',\n",
       "              'first', 'days', 'in', 'may', 'meanwhile', 'i', 'was', 'occupying',\n",
       "              'myself', 'in', 'classifying', 'my', 'mineralogical', 'botanical',\n",
       "              'and', 'zoological', 'riches', 'when', 'the', 'accident',\n",
       "              'happened', 'to', 'the', 'scotia', 'i', 'was', 'perfectly', 'up',\n",
       "              'in', 'the', 'subject', 'which', 'was', 'the', 'question', 'of',\n",
       "              'the', 'day', 'how', 'could', 'i', 'be', 'otherwise', 'i', 'had',\n",
       "              'read', 'and', 're', 'read', 'all', 'the', 'american', 'and',\n",
       "              'european', 'papers', 'without', 'being', 'any', 'nearer', 'a',\n",
       "              'conclusion', 'this', 'mystery', 'puzzled', 'me', 'under', 'the',\n",
       "              'impossibility', 'of', 'forming', 'an', 'opinion', 'i', 'jumped',\n",
       "              'from', 'one', 'extreme', 'to', 'the', 'other'], dtype='<U13')     ,\n",
       "       array(['captain', 'baker', 'thought', 'at', 'first', 'that', 'he', 'was',\n",
       "              'in', 'the', 'presence', 'of', 'an', 'unknown', 'sandbank', 'he',\n",
       "              'even', 'prepared', 'to', 'determine', 'its', 'exact', 'position',\n",
       "              'when', 'two', 'columns', 'of', 'water', 'projected', 'by', 'the',\n",
       "              'mysterious', 'object', 'shot', 'with', 'a', 'hissing', 'noise',\n",
       "              'a', 'hundred', 'and', 'fifty', 'feet', 'up', 'into', 'the', 'air',\n",
       "              'now', 'unless', 'the', 'sandbank', 'had', 'been', 'submitted',\n",
       "              'to', 'the', 'intermittent', 'eruption', 'of', 'a', 'geezer',\n",
       "              'the', 'governor', 'higginson', 'had', 'to', 'do', 'neither',\n",
       "              'more', 'nor', 'less', 'than', 'with', 'an', 'aquatic', 'mammal',\n",
       "              'unknown', 'till', 'then', 'which', 'threw', 'up', 'from', 'its',\n",
       "              'blowholes', 'columns', 'of', 'water', 'mixed', 'with', 'air',\n",
       "              'and', 'vapour', 'similar', 'facts', 'were', 'observed', 'on',\n",
       "              'the', '23rd', 'of', 'july', 'in', 'the', 'same', 'year', 'in',\n",
       "              'the', 'pacific', 'ocean', 'by', 'the', 'columbus', 'of', 'the',\n",
       "              'west', 'india', 'and', 'pacific', 'steam', 'navigation',\n",
       "              'company', 'but', 'this', 'extraordinary', 'creature', 'could',\n",
       "              'transport', 'itself', 'from', 'one', 'place', 'to', 'another',\n",
       "              'with', 'surprising', 'velocity', 'as', 'in', 'an', 'interval',\n",
       "              'of', 'three', 'days', 'higginson', 'columbus', 'had', 'observed',\n",
       "              'it', 'at', 'two', 'different', 'points', 'of', 'the', 'chart',\n",
       "              'separated', 'by', 'a', 'distance', 'of', 'more', 'than', '700',\n",
       "              'nautical', 'leagues'], dtype='<U13')                              ,\n",
       "       array(['increase', 'its', 'size', 'fivefold', 'or', 'tenfold', 'give',\n",
       "              'it', 'strength', 'proportionate', 'to', 'its', 'size',\n",
       "              'destructive', 'weapons', 'and', 'you', 'obtain', 'the', 'animal',\n",
       "              'required', 'it', 'will', 'have', 'the', 'proportions',\n",
       "              'determined', 'by', 'the', 'officers', 'of', 'the', 'shannon',\n",
       "              'the', 'instrument', 'required', 'by', 'the', 'perforation', 'of',\n",
       "              'the', 'scotia', 'and', 'the', 'power', 'necessary', 'to',\n",
       "              'pierce', 'the', 'hull', 'of', 'the', 'steamer', 'indeed', 'the',\n",
       "              'narwhal', 'is', 'armed', 'with', 'a', 'sort', 'of', 'ivory',\n",
       "              'sword', 'a', 'halberd', 'according', 'to', 'the', 'expression',\n",
       "              'of', 'certain', 'naturalists', 'principal', 'tusk', 'has', 'the',\n",
       "              'hardness', 'of', 'steel', 'some', 'of', 'these', 'tusks', 'have',\n",
       "              'been', 'found', 'buried', 'in', 'the', 'bodies', 'of', 'whales',\n",
       "              'which', 'the', 'unicorn', 'always', 'attacks', 'with', 'success',\n",
       "              'others', 'have', 'been', 'drawn', 'out', 'not', 'without',\n",
       "              'trouble', 'from', 'the', 'bottoms', 'of', 'ships', 'which',\n",
       "              'they', 'had', 'pierced', 'through', 'and', 'through', 'as', 'a',\n",
       "              'gimlet', 'pierces', 'a', 'barrel', 'the', 'museum', 'of', 'the',\n",
       "              'faculty', 'of', 'medicine', 'of', 'paris', 'possesses', 'one',\n",
       "              'of', 'these', 'defensive', 'weapons', 'two', 'yards', 'and', 'a',\n",
       "              'quarter', 'in', 'length', 'and', 'fifteen', 'inches', 'in',\n",
       "              'diameter', 'at', 'the', 'base'], dtype='<U13')                   ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36851cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_text=[w for text in words for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03b8475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [[\"the\",\"dog\",\"is\",\"a\",\"man's\",\"best\",\"friend\"],[\"but\",\"the\",\"cat\",\"is\",\"a\",\"woman's\",\".\"]]\n",
    "sliding=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d05b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nWindows = max(len(whole_text)-sliding + 1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "398db592",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = np.empty((nWindows))\n",
    "for i in range(nWindows):\n",
    "    i=0\n",
    "    window_text = whole_text[i:(i+sliding)]\n",
    "    window_text_joined = \" \".join(window_text)\n",
    "    masked_window = mask_array(window_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "817f28bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<mask>',\n",
       " 'the <mask>',\n",
       " 'the dog <mask>',\n",
       " 'the dog is <mask>',\n",
       " 'the dog is a <mask>',\n",
       " \"the dog is a man's <mask>\",\n",
       " \"the dog is a man's best <mask>\"]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "37b8f895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the dog is a man's best friend\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_text_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "97995b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'dog', 'is', 'a', \"man's\", 'best', 'friend']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8108e95f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mtarget_tokens\u001b[49m[mask_index[\u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(target_tokens[mask_index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b40d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRobertaProbabilities(batch,truth):\n",
    "    #tokenize the masked batch\n",
    "    tokens = torch.tensor(tokenizer(batch,padding=True)['input_ids'])\n",
    "    #tokenize the ground truth\n",
    "    target_tokens = torch.tensor(tokenizer(\" \".join(truth))['input_ids'])\n",
    "    \n",
    "    #where in each token batch is each mask?\n",
    "    row,mask_index = (tokens == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "    #get list of ground truth token values for each mask position\n",
    "    target_tokens_in_mask = target_tokens[mask_index]\n",
    "    \n",
    "    #predict tokens\n",
    "    predicted_tokens = roberta(tokens)[0]\n",
    "    #keep only the prediction at mask\n",
    "    predicted_vectors = predicted_tokens[row,mask_index,:]\n",
    "    #apply softmax\n",
    "    probs = predicted_vectors.softmax(dim=1)\n",
    "    maxProb = torch.max(probs)\n",
    "    probs = probs[:,target_tokens_in_mask]\n",
    "    probs_norm = probs/maxProb\n",
    "    return(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ba48c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the masked batch\n",
    "tokens = torch.tensor(tokenizer(batch,padding=True)['input_ids'])\n",
    "#tokenize the ground truth\n",
    "target_tokens = torch.tensor(tokenizer(truth)['input_ids'])\n",
    "\n",
    "#where in each token batch is each mask?\n",
    "row,mask_index = (tokens == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "#get list of ground truth token values for each mask position\n",
    "target_tokens_in_mask = target_tokens[mask_index]\n",
    "\n",
    "#predict tokens\n",
    "predicted_tokens = roberta(tokens)[0]\n",
    "#keep only the prediction at mask\n",
    "predicted_vectors = predicted_tokens[row,mask_index,:]\n",
    "#apply softmax\n",
    "probs = predicted_vectors.softmax(dim=0)\n",
    "maxProb = torch.max(probs)\n",
    "probs_targets = [ probs[i,j] for i,j in zip(row,target_tokens_in_mask)]\n",
    "probs_norm = probs_targets/maxProb\n",
    "return(probs_targets,probs_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "14e6d78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(probs,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb938e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "96987a24",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m maxProb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(probs)\n\u001b[1;32m     19\u001b[0m probs_targets \u001b[38;5;241m=\u001b[39m [ probs[i,j] \u001b[38;5;28;01mfor\u001b[39;00m i,j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(row,target_tokens_in_mask)]\n\u001b[0;32m---> 20\u001b[0m probs_norm \u001b[38;5;241m=\u001b[39m \u001b[43mprobs_targets\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mmaxProb\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "batch = [\"he dog is a man's best <mask>\"]\n",
    "truth = [\"he dog is a man's best friend\"]\n",
    "tokens = torch.tensor(tokenizer(batch)['input_ids'])\n",
    "#tokenize the ground truth\n",
    "target_tokens = torch.tensor(tokenizer(truth)['input_ids'])\n",
    "\n",
    "#where in each token batch is each mask?\n",
    "row,mask_index = (tokens == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "#get list of ground truth token values for each mask position\n",
    "target_tokens_in_mask = target_tokens[0,mask_index]\n",
    "\n",
    "#predict tokens\n",
    "predicted_tokens = roberta(tokens)[0]\n",
    "#keep only the prediction at mask\n",
    "predicted_vectors = predicted_tokens[row,mask_index,:]\n",
    "#apply softmax\n",
    "probs = predicted_vectors.softmax(dim=1)\n",
    "maxProb = torch.max(probs)\n",
    "probs_targets = torch.tensor([ probs[i,j] for i,j in zip(row,target_tokens_in_mask)])\n",
    "probs_norm = probs_targets/maxProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "38c666f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_targets = torch.tensor([ probs[i,j] for i,j in zip(row,target_tokens_in_mask)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cdf95bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxProb = torch.max(probs)\n",
    "probs_targets = torch.tensor([ probs[i,j] for i,j in zip(row,target_tokens_in_mask)])\n",
    "probs_norm = probs_targets/maxProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "59825d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_norm.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "792fc32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f25f90a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 627, 2335,   16,   10,  313,  275, 1441])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens_in_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c2982562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 50264,     2,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    0,   627, 50264,     2,     1,     1,     1,     1,     1,     1],\n",
       "        [    0,   627,  2335, 50264,     2,     1,     1,     1,     1,     1],\n",
       "        [    0,   627,  2335,    16, 50264,     2,     1,     1,     1,     1],\n",
       "        [    0,   627,  2335,    16,    10, 50264,     2,     1,     1,     1],\n",
       "        [    0,   627,  2335,    16,    10,   313,    18, 50264,     2,     1],\n",
       "        [    0,   627,  2335,    16,    10,   313,    18,   275, 50264,     2]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "036d12a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<mask>',\n",
       " 'the <mask>',\n",
       " 'the dog <mask>',\n",
       " 'the dog is <mask>',\n",
       " 'the dog is a <mask>',\n",
       " \"the dog is a man's <mask>\",\n",
       " \"the dog is a man's best <mask>\"]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "796c3a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "9972ff5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4337e-08, 8.0403e-05, 7.5938e-08, 1.1414e-03, 1.0345e-04, 7.1672e-03,\n",
       "        8.8760e-04, 1.0130e-02, 1.9647e-03, 5.3256e-04, 4.6949e-04, 5.3233e-03,\n",
       "        6.3513e-04, 1.9747e-02, 1.8745e-04, 1.6798e-04, 2.7663e-05, 4.7144e-04,\n",
       "        5.5107e-03, 8.2309e-06, 2.3819e-03, 5.1805e-04, 8.8909e-06, 3.0148e-05,\n",
       "        2.3770e-05, 2.3330e-03, 1.1841e-06, 3.2732e-05, 7.9461e-06, 4.5775e-02,\n",
       "        2.4344e-04, 2.6501e-05, 6.2112e-06, 5.4425e-06, 2.6775e-05, 1.2970e-05,\n",
       "        1.4905e-04, 3.5645e-04, 4.5467e-06, 1.1616e-05, 2.9712e-06, 1.3558e-05,\n",
       "        2.2583e-08, 3.7958e-04, 6.6369e-05, 5.7872e-07, 1.7677e-07, 2.2536e-06,\n",
       "        1.6873e-06, 1.4904e-05], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[row,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2d08250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3129, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(predicted_vectors.softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b1da0a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd179a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certainly might possess such a destructive machine and in these disastrous times when the ingenuity of man has multiplied the power of weapons of war it was possible that without the knowledge of others a state to work such a formidable engine the idea of a war machine fell before the declaration of governments as public interest was in question and transatlantic communications their veracity could not be doubted but how admit that the construction of this submarine boat had escaped the public eye for a private gentleman to keep the secret under such circumstances would be very difficult and for a state whose every act is persistently watched by powerful rivals certainly impossible upon my arrival in new york several persons did me the honor of consulting the phenomenon in question i had published in france a work in two volumes entitled mysteries of the great submarine grounds'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_markings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2afbdd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jv-pln]",
   "language": "python",
   "name": "conda-env-jv-pln-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
